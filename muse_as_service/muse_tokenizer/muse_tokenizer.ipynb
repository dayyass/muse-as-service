{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e996712f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from muse_as_service import download_thhub_model\n",
    "from tokenizer import get_tokenizer_from_saved_model, parse_saved_model, tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f62d33d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# init variable\n",
    "thhub_model_url = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\"\n",
    "save_model_path = \".cache/universal-sentence-encoder-multilingual_3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7b6bab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading '.tar' model ...\n",
      "unpacking '.tar' model ...\n"
     ]
    }
   ],
   "source": [
    "# load and unpack model\n",
    "download_thhub_model(\n",
    "    thhub_model_url=thhub_model_url,\n",
    "    save_model_path=save_model_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5207ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tokenizer\n",
    "tokenizer = get_tokenizer_from_saved_model(\n",
    "    parse_saved_model(save_model_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380abd77",
   "metadata": {},
   "source": [
    "### use tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4fd4212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some texts of different lengths.\n",
    "english_sentences = [\"dog\", \"Puppies are nice.\", \"I enjoy taking long walks along the beach with my dog.\"]\n",
    "italian_sentences = [\"cane\", \"I cuccioli sono carini.\", \"Mi piace fare lunghe passeggiate lungo la spiaggia con il mio cane.\"]\n",
    "japanese_sentences = [\"犬\", \"子犬はいいです\", \"私は犬と一緒にビーチを散歩するのが好きです\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26ed85da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dog -> ['▁dog']\n",
      "Puppies are nice. -> ['▁Pupp', 'ies', '▁are', '▁nice', '.']\n",
      "I enjoy taking long walks along the beach with my dog. -> ['▁I', '▁enjoy', '▁taking', '▁long', '▁walk', 's', '▁along', '▁the', '▁beach', '▁with', '▁my', '▁dog', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in english_sentences:\n",
    "    tokenized_sentence = tokenize(\n",
    "        sentence=sentence,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    print(f\"{sentence} -> {tokenized_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d788e5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cane -> ['▁cane']\n",
      "I cuccioli sono carini. -> ['▁I', '▁cu', 'ccioli', '▁sono', '▁car', 'ini', '.']\n",
      "Mi piace fare lunghe passeggiate lungo la spiaggia con il mio cane. -> ['▁Mi', '▁piace', '▁fare', '▁lunghe', '▁passeggiat', 'e', '▁lungo', '▁la', '▁spiaggia', '▁con', '▁il', '▁mio', '▁cane', '.']\n"
     ]
    }
   ],
   "source": [
    "for sentence in italian_sentences:\n",
    "    tokenized_sentence = tokenize(\n",
    "        sentence=sentence,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    print(f\"{sentence} -> {tokenized_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9664c90f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "犬 -> ['▁', '犬']\n",
      "子犬はいいです -> ['▁', '子', '犬', 'は', 'いい', 'です']\n",
      "私は犬と一緒にビーチを散歩するのが好きです -> ['▁私', 'は', '犬', 'と一緒に', 'ビーチ', 'を', '散', '歩', 'するのが', '好き', 'です']\n"
     ]
    }
   ],
   "source": [
    "for sentence in japanese_sentences:\n",
    "    tokenized_sentence = tokenize(\n",
    "        sentence=sentence,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    print(f\"{sentence} -> {tokenized_sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883211e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
